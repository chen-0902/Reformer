{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from reformer_pytorch import ReformerLM\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_HEADS = 4\n",
    "EMEBED_DIM = 256\n",
    "TRAIN_EPOCHS = 6\n",
    "seq_length = 256\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "# Reload the training iterator for building vocab\n",
    "train_iter, _ = IMDB(split=(\"train\", \"test\"))\n",
    "vocab = build_vocab_from_iterator(yield_tokens(\n",
    "    train_iter), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(\n",
    "    vocab[\"<unk>\"]\n",
    ")  # Set default index for out-of-vocabulary tokens\n",
    "NUM_TOKENS = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(x):\n",
    "    return vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, d_model=256, nhead=4, num_encoder_layers=1, dim_feedforward=256):\n",
    "        super(EncoderOnlyTransformer, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            num_layers=num_encoder_layers,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.embedding = nn.Embedding(NUM_TOKENS, d_model)\n",
    "        self.classifier = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.embedding(src)\n",
    "        encoded_output = self.transformer_encoder(src)\n",
    "        # encoded_output = encoded_output[:, -1, :]\n",
    "        encoded_output = self.dropout(encoded_output)\n",
    "        encoded_output = encoded_output.max(dim=1)[0]\n",
    "        output = self.classifier(encoded_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderOnlyTransformer(\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (embedding): Embedding(100684, 256)\n",
       "  (classifier): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"transformer_model_weights.pth\"\n",
    "transformer = EncoderOnlyTransformer(\n",
    "    d_model=EMEBED_DIM,\n",
    "    nhead=NUM_HEADS,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    dim_feedforward=EMEBED_DIM,\n",
    ").to(device)\n",
    "transformer.load_state_dict(torch.load(model_path))\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerForClassification(nn.Module):\n",
    "    def __init__(self, num_tokens, emb_dim, dim, depth, heads, max_seq_len):\n",
    "        super(ReformerForClassification, self).__init__()\n",
    "        self.encoder = ReformerLM(\n",
    "            num_tokens=num_tokens,\n",
    "            emb_dim=emb_dim,\n",
    "            dim=dim,\n",
    "            depth=depth,\n",
    "            heads=heads,\n",
    "            max_seq_len=max_seq_len,\n",
    "            fixed_position_emb=True,\n",
    "            return_embeddings=True,\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder processes the input\n",
    "        encoded_output = self.encoder(x)\n",
    "        encoded_output = self.dropout(encoded_output)\n",
    "        encoded_output = encoded_output.max(dim=1)[0]\n",
    "        output = self.classifier(encoded_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerForClassification(\n",
       "  (encoder): ReformerLM(\n",
       "    (token_emb): Embedding(100684, 256)\n",
       "    (to_model_dim): Identity()\n",
       "    (pos_emb): FixedPositionalEmbedding()\n",
       "    (layer_pos_emb): Always()\n",
       "    (reformer): Reformer(\n",
       "      (layers): ReversibleSequence(\n",
       "        (blocks): ModuleList(\n",
       "          (0-2): 3 x ReversibleBlock(\n",
       "            (f): Deterministic(\n",
       "              (net): PreNorm(\n",
       "                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): LSHSelfAttention(\n",
       "                  (toqk): Linear(in_features=256, out_features=256, bias=False)\n",
       "                  (tov): Linear(in_features=256, out_features=256, bias=False)\n",
       "                  (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (lsh_attn): LSHAttention(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (full_attn): FullQKAttention(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (local_attn): LocalAttention(\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (g): Deterministic(\n",
       "              (net): PreNorm(\n",
       "                (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Chunk(\n",
       "                  (fn): FeedForward(\n",
       "                    (w1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                    (act): GELU(approximate='none')\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (w2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (irrev_blocks): ModuleList(\n",
       "          (0-2): 3 x IrreversibleBlock(\n",
       "            (f): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): LSHSelfAttention(\n",
       "                (toqk): Linear(in_features=256, out_features=256, bias=False)\n",
       "                (tov): Linear(in_features=256, out_features=256, bias=False)\n",
       "                (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (lsh_attn): LSHAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (dropout_for_hash): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (full_attn): FullQKAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (post_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (local_attn): LocalAttention(\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (g): PreNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Chunk(\n",
       "                (fn): FeedForward(\n",
       "                  (w1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (w2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (out): Identity()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"reformer_model_weights.pth\"\n",
    "reformer = ReformerForClassification(\n",
    "    num_tokens=NUM_TOKENS,\n",
    "    emb_dim=EMEBED_DIM,\n",
    "    dim=EMEBED_DIM,\n",
    "    depth=NUM_ENCODER_LAYERS,\n",
    "    heads=NUM_HEADS,\n",
    "    max_seq_len=seq_length,\n",
    ").to(device)\n",
    "reformer.load_state_dict(torch.load(model_path))\n",
    "reformer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model):\n",
    "    if model == \"Reformer\":\n",
    "        model = reformer\n",
    "    else:\n",
    "        model = transformer\n",
    "    tokens = torch.tensor(text_pipeline(text), dtype=torch.int64).to(device)\n",
    "    output = model(tokens.unsqueeze(0))\n",
    "    output = torch.sigmoid(output).item()\n",
    "    y_pred = \"Positive ü•≥ ü§© üëç\" if output > 0.5 else \"Negative ü§Æ üôÖ üëé\"\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaolongli/anaconda3/envs/pytorch/lib/python3.11/site-packages/gradio/interface.py:331: UserWarning: The `allow_flagging` parameter in `Interface` nowtakes a string value ('auto', 'manual', or 'never'), not a boolean. Setting parameter to: 'never'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "IMPORTANT: You are using gradio version 3.48.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\"Reformer\", \"Transformer\"]\n",
    "demo = gr.Interface(\n",
    "    fn=inference,\n",
    "    inputs=[\n",
    "        \"text\",\n",
    "        gr.CheckboxGroup(\n",
    "            models,\n",
    "            label=\"Model Selection\",\n",
    "            info=\"Which model would you like to use?\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=gr.Textbox(\n",
    "        label=\"Results\",\n",
    "        lines=1,\n",
    "        show_copy_button=True,\n",
    "        show_label=True,\n",
    "        placeholder=\"üì¢‚ùóüö®Results will be displayed here.\",\n",
    "    ),\n",
    "    allow_flagging=False,\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
